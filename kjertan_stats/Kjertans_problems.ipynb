{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solve this?:\n",
    "    $$ \\frac{acc_{direction}}{\\sin(\\theta+\\Delta\\theta)} = \\frac{acc_{reverse}}{\\sin(\\theta-\\Delta\\theta)}$$\n",
    "    \n",
    "- vi denne lighed hvor vi prøver at gøre delta theta så lille som mulight ved at difference nedenfor er mindre end (0.000001).\n",
    "- begge accelerationer, og theta er normalfordelinger med en sigma, mens deltatheta er ukendt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code as is (more or less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference = 0.2732218764830747, dtheta= 0.9918033437055077, iteration = 0\n",
      "Difference = 0.02304665551825824, dtheta= 0.7233621350392422, iteration = 100\n",
      "Difference = 0.002988350759054903, dtheta= 0.6943838305219826, iteration = 200\n",
      "Difference = 0.0004001658036272282, dtheta= 0.6905635638321012, iteration = 300\n",
      "Difference = 5.380336313054457e-05, dtheta= 0.6900509431806945, iteration = 400\n",
      "Difference = 7.237919107394575e-06, dtheta= 0.6899820010736606, iteration = 500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = np.random\n",
    "r.seed(42)\n",
    "\n",
    "\n",
    "#N_exp =1000\n",
    "optimize_max_steps=1000\n",
    "print_steps = optimize_max_steps/10\n",
    "\n",
    "for iexp in range(N_exp):\n",
    "    # Generate random experiment?\n",
    "    a_norm = r.normal(1.5, 0.1)\n",
    "    a_rev = r.normal(1.4, 0.1)\n",
    "    # Why is theta different. \n",
    "    # isn't theta (the inclination of the table the same)\n",
    "    # and we try to find a Delta theta that will align it better \n",
    "    # (ie. correct for the accellation difference)?\n",
    "    theta_norm = r.normal(13.4, 0.2)\n",
    "    theta_rev = r.normal(13.9,0.3)\n",
    "\n",
    "    diff = 1.0\n",
    "    # These are in radian ie. sin(3.14) = 0, sin(3.14/2 = 1)\n",
    "    dtheta= 1\n",
    "    ddtheta = 0.03\n",
    "    N=0\n",
    "    diff_new = 1\n",
    "    while (diff_new > 1e-6):\n",
    "        # calc both sides of equation\n",
    "        norm = a_norm/np.sin(theta_norm+dtheta)\n",
    "        rev = a_rev/np.sin(theta_rev-dtheta)\n",
    "        # find diference\n",
    "        diff_new = abs(norm-rev)\n",
    "        # find ratio of difference compared to original distance\n",
    "        # Quite unorthodox but seems to work :)\n",
    "        ratio = diff_new/diff\n",
    "        # subtract a smaller and smaller amount\n",
    "        # Note since diff_abs is always positive you only subtract values from it\n",
    "        # Since sinus is cyclical it works in this case but would not generalize\n",
    "        dtheta -= ddtheta*ratio\n",
    "\n",
    "        #diff = diff_new\n",
    "\n",
    "\n",
    "        \n",
    "        if iexp == 0 and N%print_steps == 0:\n",
    "            print(f'Difference = {diff_new}, dtheta= {dtheta}, iteration = {N}')\n",
    "        N += 1\n",
    "        \n",
    "        if N==optimize_max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sin(np.pi/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emil notes\n",
    "The \"proper\" way to solve optimization is to frase it as a minimization problem\n",
    "\n",
    "$$ \\frac{acc_{direction}}{\\sin(\\theta+\\Delta\\theta)} = \\frac{acc_{reverse}}{\\sin(\\theta-\\Delta\\theta)}$$\n",
    "- becomes this\n",
    "$$ f(\\Delta\\theta) = \\frac{acc_{direction}}{\\sin(\\theta+\\Delta\\theta)} - \\frac{acc_{reverse}}{\\sin(\\theta-\\Delta\\theta)} = 0$$\n",
    "- will be fullfiled if we minimize\n",
    "$$ min(f(\\Delta\\theta)^2)_{\\Delta\\theta}$$\n",
    "- then you can use standard methods such as \n",
    "    - Newton rapton: https://en.wikipedia.org/wiki/Newton's_method\n",
    "    - gradient decent: https://en.wikipedia.org/wiki/Gradient_descent\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emils meta notes\n",
    "- You probably should only one $\\theta$ (and not both $\\theta_{norm}$ and $\\theta_{reverse}$) (you only have 1 table)\n",
    "- I am not sure you are solving the right problem. Ie. I suspect it would be more around hypothesis testing so given different accelations generate some distributions and do calculus / simulations with the distributions \n",
    "\n",
    "(you seem to be solving the same problem a 1000 times with different random generated values. but I could be wrong :)\n",
    "- suspected real approach\n",
    "    - 1. generate 1000 data points for each distribution\n",
    "    - 2. Calculate the means and std_of_means \n",
    "    - 3. Given these calculate the possible ranges for $\\Delta\\theta$ once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
